tosca_definitions_version: tosca_simple_yaml_1_0

imports:
  - grycap_custom_types: https://raw.githubusercontent.com/grycap/tosca/main/custom_types.yaml

metadata:
  template_version: "1.0.0"
  template_name: Ollama + Open WebUI
  display_name: Install Ollama + Open WebUI

description: TOSCA template for deploying a Ollama + Open WebUI VM

topology_template:

  inputs:
    num_cpus:
      type: integer
      description: Number of virtual cpus for the VM
      default: 8
    mem_size:
      type: scalar-unit.size
      description: Amount of memory for the VM
      default: 64 GB
    disk_size:
      type: scalar-unit.size
      description: Size of the root disk of the VM (in case of 0, the disk will not be resized)
      default: 100 GB

    num_gpus:
      type: integer
      description: Number of GPUs to assing to this VM
      default: 0

    ollama_nvidia_support:
      type: boolean
      description: Flag to add the NVIDIA runtime to the Ollama installation
      default: false
      constraints:
        - valid_values: [ false, true ]

    ollama_driver_version:
      type: string
      description: NVIDIA Driver version to install
      default: "535"

    ollama_models_name:
      type: list
      entry_schema:
        type: string
      description: Name of the AI models to install (see https://ollama.com/models)
      default: ["mistral", "llama3"]
      required: yes

  node_templates:

    ollama:
      type: tosca.nodes.ec3.Application
      requirements:
        - host: simple_node
      artifacts:
        docker_role:
          file: grycap.docker
          type: tosca.artifacts.AnsibleGalaxy.role
      capabilities:
        endpoint:
          properties:
            ports:
              https:
                protocol: tcp
                source: 443
      interfaces:
        Standard:
          configure:
            implementation: https://raw.githubusercontent.com/grycap/tosca/main/artifacts/ollama.yml
            inputs:
              ollama_nvidia_support: { get_input: ollama_nvidia_support }
              ollama_driver_version: { get_input: ollama_driver_version }
              ollama_models_name: { get_input: ollama_models_name }
              node_public_address: { get_attribute: [ simple_node, public_address, 0 ] }
              dns_node_name: { concat: [ "ollama.", { get_attribute: [ simple_node, public_address, 0 ] }, ".nip.io/"] }

    simple_node:
      type: tosca.nodes.indigo.Compute
      capabilities:
        endpoint:
          properties:
            network_name: PUBLIC
        host:
          properties:
            disk_size: { get_input: disk_size }
            num_cpus: { get_input: num_cpus }
            mem_size: { get_input: mem_size }
            num_gpus: { get_input: num_gpus }
        os:
          properties:
            type: linux
            distribution: ubuntu

  outputs:
    openwebui_url:
      value: { concat: [ "https://ollama.", { get_attribute: [ simple_node, public_address, 0 ] }, ".nip.io/"] }
    node_ip:
      value: { get_attribute: [ simple_node, public_address, 0 ] }
    node_creds:
      value: { get_attribute: [ simple_node, endpoint, credential, 0 ] }
