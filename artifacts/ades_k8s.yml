---
- hosts: localhost
  connection: local
  vars:
    version: "{{ ades_version | default('master') }}"
    namespace: "{{ ades_namespace | default('eoepca') }}"
    public_ip_address: "{{ IM_NODE_PUBLIC_IP | default(ansible_default_ipv4.address) }}"
    rabbitmq_username: "{{ ades_rabbitmq_username | default('rabbitmq_username') }}"
    rabbitmq_password: "{{ ades_rabbitmq_password | default('rabbitmq_password') }}"
    minio_accesskey: "{{ ades_minio_accesskey | default('minio_accesskey') }}"
    minio_secretkey: "{{ ades_minio_secretkey | default('minio_secretkey') }}"
  tasks:
    - name: Create MinIO Helm values file
      copy:
        dest: /tmp/minio_config.yaml
        mode: '644'
        content: |
          rootUser: "{{ minio_accesskey }}"
          rootPassword: "{{ minio_secretkey }}"
          replicas: 2
          ingress:
            enabled: true
            ingressClassName: nginx
            annotations:
              cert-manager.io/cluster-issuer: "letsencrypt-prod"
              nginx.ingress.kubernetes.io/ssl-redirect: "true"
              nginx.ingress.kubernetes.io/proxy-body-size: "0"
              nginx.ingress.kubernetes.io/proxy-read-timeout: '600'
            path: /
            hosts:
              - minio.{{ public_ip_address }}.nip.io
            tls:
              - secretName: minio-tls
                hosts:
                  - minio.{{ public_ip_address }}.nip.io
          consoleIngress:
            enabled: true
            ingressClassName: nginx
            annotations:
              cert-manager.io/cluster-issuer: "letsencrypt-prod"
              nginx.ingress.kubernetes.io/ssl-redirect: "true"
              nginx.ingress.kubernetes.io/proxy-body-size: "0"
              nginx.ingress.kubernetes.io/proxy-read-timeout: '600'
            path: /
            hosts:
              - console.minio.{{ public_ip_address }}.nip.io
            tls:
            - secretName: minio-console-tls
              hosts:
                - console.minio.{{ public_ip_address }}.nip.io
          resources:
            requests:
              memory: 1Gi
          persistence:
            storageClass: ""
          buckets:
            - name: eoepca
            - name: cache-bucket

    - name: Install minio Helm repository
      command: helm repo add minio https://charts.min.io/
      environment:
        KUBECONFIG: "/etc/kubernetes/admin.conf"
      register: helm_repo_minio
      changed_when: "'already exists' not in helm_repo_minio.stdout"

    - name: Install (or upgrade) the MinIO chart
      command: helm upgrade --install minio minio/minio --namespace minio --create-namespace --values /tmp/minio_config.yaml --timeout 10m
      environment:
        KUBECONFIG: "/etc/kubernetes/admin.conf"

    - name: Install eoepca Helm repository
      command: helm repo add eoepca https://eoepca.github.io/helm-charts/
      environment:
        KUBECONFIG: "/etc/kubernetes/admin.conf"
      register: helm_repo_eoepca
      changed_when: "'already exists' not in helm_repo_eoepca.stdout"

    - name: Create ZOO-Project DRU Helm values file
      copy:
        dest: /tmp/ades-values.yaml
        mode: '644'
        content: |
          iam:
            enabled: false
          customConfig:
            main:
              eoepca: |-
                domain={{ public_ip_address }}.nip.io
          files:
            # Directory 'files/cwlwrapper-assets' - assets for ConfigMap 'XXX-cwlwrapper-config'
            cwlwrapperAssets:
              # main.yaml: ""
              # rules.yaml: ""
              # stagein.yaml: ""
              stageout.yaml: |-
                cwlVersion: v1.0
                class: CommandLineTool
                id: stage-out
                doc: "Stage-out the results to S3"
                inputs:
                  process:
                    type: string
                  collection_id:
                    type: string
                  STAGEOUT_OUTPUT:
                    type: string
                  STAGEOUT_AWS_ACCESS_KEY_ID:
                    type: string
                  STAGEOUT_AWS_SECRET_ACCESS_KEY:
                    type: string
                  STAGEOUT_AWS_REGION:
                    type: string
                  STAGEOUT_AWS_SERVICEURL:
                    type: string
                outputs:
                  StacCatalogUri:
                    outputBinding:
                      outputEval: ${  return "s3://" + inputs.STAGEOUT_OUTPUT + "/" + inputs.process + "/catalog.json"; }
                    type: string
                baseCommand:
                  - python
                  - stageout.py
                arguments:
                  - $( inputs.wf_outputs.path )
                  - $( inputs.STAGEOUT_OUTPUT )
                  - $( inputs.process )
                  - $( inputs.collection_id )
                requirements:
                  DockerRequirement:
                    dockerPull: ghcr.io/terradue/ogc-eo-application-package-hands-on/stage:1.3.2
                  InlineJavascriptRequirement: {}
                  EnvVarRequirement:
                    envDef:
                      AWS_ACCESS_KEY_ID: $( inputs.STAGEOUT_AWS_ACCESS_KEY_ID )
                      AWS_SECRET_ACCESS_KEY: $( inputs.STAGEOUT_AWS_SECRET_ACCESS_KEY )
                      AWS_REGION: $( inputs.STAGEOUT_AWS_REGION )
                      AWS_S3_ENDPOINT: $( inputs.STAGEOUT_AWS_SERVICEURL )
                  InitialWorkDirRequirement:
                    listing:
                      - entryname: stageout.py
                        entry: |-
                          import os
                          import sys
                          import pystac
                          import botocore
                          import boto3
                          import shutil
                          from pystac.stac_io import DefaultStacIO, StacIO
                          from urllib.parse import urlparse
                          from datetime import datetime
                          cat_url = sys.argv[1]
                          bucket = sys.argv[2]
                          subfolder = sys.argv[3]
                          collection_id = sys.argv[4]
                          print(f"cat_url: {cat_url}", file=sys.stderr)
                          print(f"bucket: {bucket}", file=sys.stderr)
                          print(f"subfolder: {subfolder}", file=sys.stderr)
                          print(f"collection_id: {collection_id}", file=sys.stderr)
                          aws_access_key_id = os.environ["AWS_ACCESS_KEY_ID"]
                          aws_secret_access_key = os.environ["AWS_SECRET_ACCESS_KEY"]
                          region_name = os.environ["AWS_REGION"]
                          endpoint_url = os.environ["AWS_S3_ENDPOINT"]
                          print(f"aws_access_key_id: {aws_access_key_id}", file=sys.stderr)
                          print(f"aws_secret_access_key: {aws_secret_access_key}", file=sys.stderr)
                          print(f"region_name: {region_name}", file=sys.stderr)
                          print(f"endpoint_url: {endpoint_url}", file=sys.stderr)
                          shutil.copytree(cat_url, "/tmp/catalog")
                          cat = pystac.read_file(os.path.join("/tmp/catalog", "catalog.json"))
                          class CustomStacIO(DefaultStacIO):
                              """Custom STAC IO class that uses boto3 to read from S3."""
                              def __init__(self):
                                  self.session = botocore.session.Session()
                                  self.s3_client = self.session.create_client(
                                      service_name="s3",
                                      use_ssl=True,
                                      aws_access_key_id=aws_access_key_id,
                                      aws_secret_access_key=aws_secret_access_key,
                                      endpoint_url=endpoint_url,
                                      region_name=region_name,
                                  )
                              def write_text(self, dest, txt, *args, **kwargs):
                                  parsed = urlparse(dest)
                                  if parsed.scheme == "s3":
                                      self.s3_client.put_object(
                                          Body=txt.encode("UTF-8"),
                                          Bucket=parsed.netloc,
                                          Key=parsed.path[1:],
                                          ContentType="application/geo+json",
                                      )
                                  else:
                                      super().write_text(dest, txt, *args, **kwargs)
                          client = boto3.client(
                              "s3",
                              aws_access_key_id=aws_access_key_id,
                              aws_secret_access_key=aws_secret_access_key,
                              endpoint_url=endpoint_url,
                              region_name=region_name,
                          )
                          StacIO.set_default(CustomStacIO)
                          # create a STAC collection for the process
                          date = datetime.now().strftime("%Y-%m-%d")
                          dates = [datetime.strptime(
                              f"{date}T00:00:00", "%Y-%m-%dT%H:%M:%S"
                          ), datetime.strptime(f"{date}T23:59:59", "%Y-%m-%dT%H:%M:%S")]
                          collection = pystac.Collection(
                            id=collection_id,
                            description="description",
                            extent=pystac.Extent(
                              spatial=pystac.SpatialExtent([[-180, -90, 180, 90]]), 
                              temporal=pystac.TemporalExtent(intervals=[[min(dates), max(dates)]])
                            ),
                            title="Processing results",
                            href=f"s3://{bucket}/{subfolder}/collection.json",
                            stac_extensions=[],
                            keywords=["eoepca"],
                            license="proprietary",
                          )
                          for index, link in enumerate(cat.links):
                            if link.rel == "root":
                                cat.links.pop(index) # remove root link
                          for item in cat.get_items():
                              item.set_collection(collection)
                              collection.add_item(item)
                              for key, asset in item.get_assets().items():
                                  s3_path = os.path.normpath(
                                      os.path.join(subfolder, collection_id, item.id, os.path.basename(asset.href))
                                  )
                                  print(f"upload {asset.href} to s3://{bucket}/{s3_path}",file=sys.stderr)
                                  client.upload_file(
                                      asset.get_absolute_href(),
                                      bucket,
                                      s3_path,
                                  )
                                  asset.href = f"s3://{bucket}/{s3_path}"
                                  item.add_asset(key, asset)
                          collection.update_extent_from_items() 
                          cat.clear_items()
                          cat.add_child(collection)
                          cat.normalize_hrefs(f"s3://{bucket}/{subfolder}")
                          for item in collection.get_items():
                              # upload item to S3
                              print(f"upload {item.id} to s3://{bucket}/{subfolder}", file=sys.stderr)
                              pystac.write_file(item, item.get_self_href())
                          # upload collection to S3
                          print(f"upload collection.json to s3://{bucket}/{subfolder}", file=sys.stderr)
                          pystac.write_file(collection, collection.get_self_href())
                          # upload catalog to S3
                          print(f"upload catalog.json to s3://{bucket}/{subfolder}", file=sys.stderr)
                          pystac.write_file(cat, cat.get_self_href())
                          print(f"s3://{bucket}/{subfolder}/catalog.json", file=sys.stdout)
          cookiecutter:
            templateUrl: https://github.com/EOEPCA/eoepca-proc-service-template.git
            templateBranch: master
          ingress:
            enabled: true
            className: nginx
            hosts:
            - host: zoo-open.{{ public_ip_address }}.nip.io
              paths:
              - path: /
                pathType: ImplementationSpecific
            tls:
              - secretName: zoo-open-tls
                hosts:
                  - zoo-open.{{ public_ip_address }}.nip.io
          rabbitmq:
            auth:
              username: {{ rabbitmq_username }}
              password: {{ rabbitmq_password }}
          persistence:
            storageClass: ""
            procServicesStorageClass: ""
            tmpStorageClass: ""
          workflow:
            defaultMaxRam: 16Gi
            defaultMaxCores: 8
            storageClass: ""
            inputs:
              STAGEIN_AWS_SERVICEURL: http://data.cloudferro.com
              STAGEIN_AWS_ACCESS_KEY_ID: test
              STAGEIN_AWS_SECRET_ACCESS_KEY: test
              STAGEIN_AWS_REGION: RegionOne
              STAGEOUT_AWS_SERVICEURL: http://minio.minio:9000
              STAGEOUT_AWS_ACCESS_KEY_ID: {{ minio_accesskey }}
              STAGEOUT_AWS_SECRET_ACCESS_KEY: {{ minio_secretkey }}
              STAGEOUT_AWS_REGION: RegionOne
              STAGEOUT_OUTPUT: eoepca
          postgresql:
            primary:
              persistence:
                storageClass: ""
            readReplicas:
              persistence:
                storageClass: ""

    - name: Install (or upgrade) the ZOO-Project DRU chart
      command: helm upgrade --install --namespace {{ namespace }} --create-namespace  --version 0.2.6 --values /tmp/ades-values.yaml --repo https://zoo-project.github.io/charts/ zoo-project-dru zoo-project-dru
      environment:
        KUBECONFIG: "/etc/kubernetes/admin.conf"
